{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: RNNs\n",
    "\n",
    "Authors: Omar, Rupal, Van\n",
    "\n",
    "### Context\n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. Data was originally used to detect SMS spam.\n",
    "\n",
    "### Content\n",
    "\n",
    "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "The original dataset can be found here. The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
    "\n",
    "### Classification problem\n",
    "\n",
    "Can we use this dataset to build a prediction model that will accurately classify which texts are spam?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('spam.csv', encoding='latin1')\n",
    "# drop blank columns\n",
    "df = df.drop('Unnamed: 2',1)\n",
    "df = df.drop('Unnamed: 3',1)\n",
    "df = df.drop('Unnamed: 4',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v1 is target v2 is the text\n",
    "columns = ['v1','v2']\n",
    "df_bin = pd.DataFrame(columns = columns)\n",
    "end = df.size - 2\n",
    "# change v1 from spam ham to binary 0 1\n",
    "for i in range(0,5572):\n",
    "    if(df['v1'][i] == \"ham\"):\n",
    "        df['v1'][i] = 0\n",
    "    else:\n",
    "        df['v1'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,) (5572,)\n"
     ]
    }
   ],
   "source": [
    "X = df['v2']\n",
    "y = df['v1']\n",
    "# still need to encode y for karis\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910 For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\n"
     ]
    }
   ],
   "source": [
    "# max chars\n",
    "max_text_length = 0\n",
    "max_text = ''\n",
    "for text in X:\n",
    "    if len(text) > max_text_length:\n",
    "        max_text_length = len(text)\n",
    "        max_text = text\n",
    "print(max_text_length, max_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# RNNs with Pre-processing in Keras\n",
    "\n",
    "## Moving to 20 Newsgroups\n",
    "So we should be able to convert a new dataset into the same format as above. Let's do this from scratch, converting the 20 news groups data. \n",
    "- http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "We looked at this a while back when we created the tf-idf and bag-of-words models. This time, we are not going to get rid of the sequence of words for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8920 unique tokens. Distilled to 8920 top words.\n",
      "Shape of data tensor: (5572, 910)\n",
      "Shape of label tensor: (5572, 2)\n",
      "8920\n",
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "MAX_ART_LEN = max_text_length # maximum and minimum number of words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(y)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(5572, 910)\n",
      "<class 'numpy.ndarray'>\n",
      "(5572, 2)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'method'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.shape)\n",
    "print(type(y_ohe))\n",
    "print(y_ohe.shape)\n",
    "print(type(y))\n",
    "y_matrix = y.as_matrix\n",
    "print(type(y_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it! The representation is now:\n",
    "- each word is converted to an integer \n",
    "- each article is a series of integers that represent the correct ordering of words\n",
    "- the target is one hot encoded\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 910) (4457, 2)\n",
      "[ 3859.   598.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=y, \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = 2\n",
    "print(X_train.shape,y_train_ohe.shape)\n",
    "print(np.sum(y_train_ohe,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the embedding\n",
    "But this is going to be a more involved process. Maybe we can speed up the training by loading up a pre-trained embedding of the words?!\n",
    "\n",
    "Let's use the GloVe word embedding in keras. We will follow the example at:\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "You can download the nearly 1GB pretrained embeddings here:\n",
    "- http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Let's take a quick look at the format of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(8921, 100)\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 910, 100)          892100    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 972,702.0\n",
      "Trainable params: 80,602.0\n",
      "Non-trainable params: 892,100.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4457/4457 [==============================] - 399s - loss: 0.1875 - acc: 0.9331 - val_loss: 0.1199 - val_acc: 0.9578\n",
      "Epoch 2/3\n",
      "4457/4457 [==============================] - 498s - loss: 0.1023 - acc: 0.9688 - val_loss: 0.0959 - val_acc: 0.9641\n",
      "Epoch 3/3\n",
      "4457/4457 [==============================] - 497s - loss: 0.0903 - acc: 0.9706 - val_loss: 0.0890 - val_acc: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x158151865c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 910, 100)          892100    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 972,702.0\n",
      "Trainable params: 80,602.0\n",
      "Non-trainable params: 892,100.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rnn2 = Sequential()\n",
    "rnn2.add(embedding_layer)\n",
    "rnn2.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn2.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn2.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4457/4457 [==============================] - 471s - loss: 0.2031 - acc: 0.9265 - val_loss: 0.1168 - val_acc: 0.9583\n",
      "Epoch 2/3\n",
      "4457/4457 [==============================] - 487s - loss: 0.1085 - acc: 0.9665 - val_loss: 0.1046 - val_acc: 0.9619\n",
      "Epoch 3/3\n",
      "4457/4457 [==============================] - 488s - loss: 0.0828 - acc: 0.9736 - val_loss: 0.0833 - val_acc: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1581df62da0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=3, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
